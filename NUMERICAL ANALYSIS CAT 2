
x = Symbol('x')
def f(x):
  return x**2 + 3*x - 4
df = diff(f(x), x)
print(df)




def f(x):
  return x**2
def trapezoidal_rule(f, a, b, n):
 Args:
      f: The function to integrate.
      a: The lower bound of integration.
      b: The upper bound of integration.
      n: The number of subintervals.
  Returns:
  h = (b - a) / n  
  integral = 0.5 * (f(a) + f(b)) 
  for i in range(1, n):
    x = a + i * h
    integral += f(x)
  return integral * h 
a = 0
b = 1
n = 100  
result = trapezoidal_rule(f, a, b, n)
print("The approximate integral of f(x) from", a, "to", b, "using", n, "sub-intervals is:", result)





import numpy as np
from scipy.optimize import curve_fit
def exp_func(x, a, b):
  return a * np.exp(b * x)
x = np.linspace(0, 5, 100)  
y = exp_func(x, 2, -1)  
y += np.random.randn(len(y)) * 0.2
popt, pcov = curve_fit(exp_func, x, y)
print("Optimized parameters:", popt)
fitted_curve = exp_func(x, *popt)  
import matplotlib.pyplot as plt
plt.plot(x, y, 'o', label='Data')
plt.plot(x, fitted_curve, label='Fitted Curve')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()





import numpy as np
from sklearn.linear_model import LinearRegression
x = np.array([10,13,15,17,19])
y = np.array([2, 4, 5, 4, 5])
model = LinearRegression()
model.fit(x.reshape(-1, 1), y)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)
predicted_y = model.predict(np.array([6]))
print("Predicted value for x=6:", predicted_y[0])





import numpy as np
from scipy.interpolate import CubicSpline
x = np.array([0, 1, 2, 3])
y = np.array([1, 2, 1, 3])
spline = CubicSpline(x, y)
new_x = np.linspace(0, 3, 100) 
new_y = spline(new_x)
import matplotlib.pyplot as plt
plt.plot(x, y, 'o', label='Data Points')
plt.plot(new_x, new_y, label='Spline Interpolation')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
specific_x = 1.5
interpolated_y = spline(specific_x)
print("Interpolated value at x =", specific_x, ":", interpolated_y)





import numpy as np
def newton_divided_difference(x, y):
    Parameters:
    x: list or array of x coordinates
    y: list or array of y coordinates
    Returns:
    n = len(x)
    coefficients = [y[0]]
    divided_diff = np.zeros((n, n))
    divided_diff[:, 0] = y
    for j in range(1, n):
        for i in range(n - j):
            divided_diff[i][j] = (divided_diff[i+1][j-1] - divided_diff[i][j-1]) / (x[i+j] - x[i])
    coefficients = [divided_diff[0][j] for j in range(n)]
    return coefficients
def newton_polynomial(coeffs, x_data, x):
    Parameters:
    coeffs: coefficients from newton_divided_difference
    x_data: original x points
    x: point at which to evaluate the polynomial
    Returns:
    y: value of the polynomial at x
    n = len(x_data) - 1
    p = coeffs[n]
    for k in range(1, n + 1):
        p = coeffs[n-k] + (x - x_data[n-k])*p
    return p
x = [1, 2, 3, 4]
y = [1, 4, 9, 16]
coeffs = newton_divided_difference(x, y)
print("Coefficients of the Newton polynomial:")
for i, coeff in enumerate(coeffs):
    print(f"a{i}: {coeff}")
print("\nVerifying interpolation:")
for xi, yi in zip(x, y):
    y_interp = newton_polynomial(coeffs, x, xi)
    print(f"f({xi}) = {y_interp:.6f}, Original y = {yi}")
x_new = 2.5
y_new = newton_polynomial(coeffs, x, x_new)
print(f"\nInterpolated value at x = {x_new}: {y_new:.6f}")







X = [2.00, 4.25, 5.25, 7.81, 9.20, 10.60]  
Y = [7.2, 7.1, 6.0, 5.0, 3.5, 5.0]    
x1 = 2.00  
y1 = 7.2  
x2 = 4.25  
y2 = 7.1  
x = 4.0    
y = y1 + ((y2 - y1) / (x2 - x1)) * (x - x1)  
print(f"The value of y at x = {x} is approximately {y:.2f}")






import numpy as np
import matplotlib.pyplot as plt
def analyze_signal_fft(f1, f2, fs, duration):
    t = np.linspace(0, duration, int(fs * duration), endpoint=False)
    s = np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)
    fft_result = np.fft.fft(s)
    freqs = np.fft.fftfreq(len(t), 1/fs)
    magnitude_spectrum = np.abs(fft_result)
    plt.figure(figsize=(12, 6))
    plt.plot(freqs[:len(freqs)//2], magnitude_spectrum[:len(freqs)//2])
    plt.title('Magnitude Spectrum of the Signal')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Magnitude')
    plt.grid(True)
    plt.show() 
    return freqs, magnitude_spectrum
f1 = 50  
f2 = 120  
fs = 1000  
duration = 1
freqs, magnitude_spectrum = analyze_signal_fft(f1, f2, fs, duration)
sorted_indices = np.argsort(magnitude_spectrum)[::-1]
print("Top frequency components:")
for i in range(4): 
    freq = abs(freqs[sorted_indices[i]])
    mag = magnitude_spectrum[sorted_indices[i]]
    print(f"Frequency: {freq:.2f} Hz, Magnitude: {mag:.2f}")






def f(x): 
  return x**2
def trapezoidal_rule(f, a, b, n):
  Args:
      f: The function to integrate.
      a: The lower bound of integration.
      b: The upper bound of integration.
      n: The number of subintervals.
  Returns:
  h = (b - a) / n  
  integral = 0.5 * (f(a) + f(b))  
  for i in range(1, n):
    x = a + i * h
    integral += f(x)
  return integral * h 
a = 0
b = 1
n = 1000 
result = trapezoidal_rule(f, a, b, n)
print("The approximate integral of f(x) from", a, "to", b, "using", n, "subintervals is:", result)






import numpy as np
def lagrange_polynomial(x, y):
    n = len(x)
    def L(k, x):
        L = np.ones_like(x)
        for j in range(n):
            if j != k:
                L *= (x - x[j]) / (x[k] - x[j])
        return L
    def P(x):
        return sum(y[k] * L(k, x) for k in range(n))
    x_eval = np.linspace(min(x), max(x), 100)
    y_eval = P(x_eval)
    coeffs = np.polyfit(x_eval, y_eval, n-1)
    return coeffs[::-1]  
x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])
coefficients = lagrange_polynomial(x, y)
print("Coefficients of the Lagrange polynomial (in ascending order of powers):")
for i, coeff in enumerate(coefficients):
    print(f"x^{i}: {coeff}")
def polynomial(x, coeffs):
    return sum(coeff * x**i for i, coeff in enumerate(coeffs))
for xi, yi in zip(x, y):
    print(f"f({xi}) = {polynomial(xi, coefficients):.6f}, Original y = {yi}")






import numpy as np
def power_iteration(A, num_iterations=1000, tolerance=1e-9):
    n, _ = A.shape
    b_k = np.random.rand(n)
    for _ in range(num_iterations):
        b_k1 = np.dot(A, b_k)
        b_k1_norm = np.linalg.norm(b_k1)
        b_k = b_k1 / b_k1_norm
        if np.linalg.norm(np.dot(A, b_k) - b_k1_norm * b_k) < tolerance:
            break 
    eigenvalue = np.dot(b_k.T, np.dot(A, b_k)) / np.dot(b_k.T, b_k)
    eigenvector = b_k
    return eigenvalue, eigenvector
A = np.array([[4, 1, 1], 
              [1, 3, -1], 
              [1, -1, 2]])
eigenvalue, eigenvector = power_iteration(A)
print("Dominant Eigenvalue:", eigenvalue)
print("Corresponding Eigenvector:", eigenvector)






def qr_algorithm(A, num_iterations=1000, tolerance=1e-9):
    n, _ = A.shape
    Q = np.eye(n)
    R = A.copy()
    for _ in range(num_iterations):
        Q, R = np.linalg.qr(R @ Q)
        if np.allclose(R @ Q - Q @ np.diag(np.diag(R)), np.zeros((n, n)), atol=tolerance):
            break
    eigenvalues = np.diag(R)
    eigenvectors = Q
    return eigenvalues, eigenvectors
eigenvalues, eigenvectors = qr_algorithm(A)
print("Eigenvalues:", eigenvalues)
print("Eigenvectors:\n", eigenvectors)
